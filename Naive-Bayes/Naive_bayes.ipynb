{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Process_Data(file_name):\n",
    "    # Process data path to data/xxx/xxx\n",
    "    data = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(line.strip().split(' ../'))\n",
    "\n",
    "    # Process data to list of words\n",
    "    email_text = []\n",
    "    delimeters = r'[ ,.\\n\\s]+' # RegEx to split words\n",
    "    for datum in data:\n",
    "        with open(datum[1], errors='ignore') as file: # Data that cant be decoded are ignored\n",
    "            email_words = re.split(delimeters, file.read())\n",
    "            # Filter the data to unique words only containing a-zA-Z and making all lowercase\n",
    "            lower_email = list(set([word.lower() for word in email_words if re.match('^[a-zA-Z]+$', word)]))\n",
    "            email_text.append(lower_email)\n",
    "\n",
    "    # Remove empty entries\n",
    "    D = [] # List of words in list\n",
    "    omega = [] # labels\n",
    "    for text, label in zip(email_text, data):\n",
    "        if text != []:\n",
    "            D.append(text)\n",
    "            omega.append(label[0])\n",
    "    return D, omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test_Split(D, omega):    \n",
    "    # Randomize the data and split to 70-30\n",
    "    permute = np.random.permutation(len(omega)) # Random index\n",
    "    split_index = round(len(omega) * 0.7)\n",
    "    D_train = []\n",
    "    omega_train = []\n",
    "    D_test = []\n",
    "    omega_test = []\n",
    "    for index, perm in enumerate(permute):\n",
    "        if index < split_index:\n",
    "            D_train.append(D[perm])\n",
    "            omega_train.append(omega[perm])\n",
    "        else:\n",
    "            D_test.append(D[perm])\n",
    "            omega_test.append(omega[perm])\n",
    "    return D_train, omega_train, D_test, omega_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab(vocab, X):\n",
    "    # Count the occurence of vocabulary words in the data\n",
    "    words, counts = np.unique(X, return_counts=True) # Unique words in data and its count\n",
    "    word_counts = np.zeros(len(vocab), dtype=int)\n",
    "    for word, count in zip(words, counts):\n",
    "        word_counts[np.where(vocab == word)] = count # np.where gets the index of the word in the vocabulary\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Naive_Bayes(D_train, omega_train, vocab=None):\n",
    "    # Training of the algorithm\n",
    "    # Note: likelihood computations are in the predict function to avoid repeating training steps when changing lambda)\n",
    "    omega = np.array(omega_train) # Training labels\n",
    "    D = np.array(D_train, dtype=object) # Training data converted\n",
    "    N = omega.shape[0] # Number of training examples\n",
    "    \n",
    "    # Get the unique words or vocabulary of the training set\n",
    "    if vocab is None:\n",
    "        vocab = np.array(sorted(list(set(word for sublist in D for word in sublist))), dtype=str)\n",
    "    V = len(vocab)\n",
    "\n",
    "    # Compute prior probabilities\n",
    "    n_spam = np.sum(omega == 'spam')\n",
    "    n_ham = np.sum(omega == 'ham')\n",
    "    prior_spam = n_spam/N\n",
    "    prior_ham = n_ham/N\n",
    "\n",
    "    # Compute the occurence of spam and ham words in each document\n",
    "    D_spam = D[omega == 'spam']\n",
    "    D_ham = D[omega == 'ham']\n",
    "    X_spam = np.concatenate(D_spam) # Convert to a 1D array\n",
    "    X_ham = np.concatenate(D_ham)\n",
    "    words_spam = count_vocab(vocab, X_spam)\n",
    "    words_ham = count_vocab(vocab, X_ham)\n",
    "    print('V:', V)\n",
    "    print('Prior Probability Spam:', prior_spam)\n",
    "    print('Prior Probability Ham:', prior_ham)\n",
    "    return prior_spam, prior_ham, words_spam, words_ham, n_spam, n_ham, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_Vocab(D_test, vocab):\n",
    "    # Return a boolean matrix of vocabulary positioning of words present in the testing data\n",
    "    D_test_bool = []\n",
    "    for d in D_test:\n",
    "        words = np.isin(vocab, d)\n",
    "        D_test_bool.append(words)\n",
    "    return D_test_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Naive_Bayes(D_test_bool, omega_test, prior_spam, prior_ham, words_spam, words_ham, n_spam, n_ham, vocab, smoothing=1):\n",
    "    # Predict the labels of the testing set\n",
    "    V = len(vocab)\n",
    "    likelihood_spam = (words_spam + smoothing)/(n_spam + smoothing*V)\n",
    "    likelihood_ham = (words_ham + smoothing)/(n_ham + smoothing*V)\n",
    "    # Log likelihood\n",
    "    log_spam_1 = np.log(likelihood_spam)\n",
    "    log_spam_0 = np.log(1 - likelihood_spam)\n",
    "    log_ham_1 = np.log(likelihood_ham)\n",
    "    log_ham_0 = np.log(1 - likelihood_ham)\n",
    "    actual = np.array(omega_test, dtype=str)\n",
    "    predictions = np.empty((0,), dtype=str)\n",
    "    for words in D_test_bool:\n",
    "        # np.where acts as vectorized 'if' statement where elements in log_1 or log_0 is used depending on the words present in the example\n",
    "        log_prob_spam = np.sum(np.where(words == 1, log_spam_1, log_spam_0)) + np.log(prior_spam)\n",
    "        log_prob_ham = np.sum(np.where(words == 1, log_ham_1, log_ham_0)) + np.log(prior_ham)\n",
    "        prediction = np.where(log_prob_spam > log_prob_ham, 'spam', 'ham') # Gets the maximum log likelihood\n",
    "        predictions = np.concatenate((predictions, prediction.reshape(1,)))\n",
    "    return actual, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_Metrics(actual, predictions):\n",
    "    # Computes the evaluation metrics\n",
    "    TP = np.sum(np.all((actual=='spam', actual==predictions), axis=0)) # Spam, classified as Spam\n",
    "    TN = np.sum(np.all((actual=='ham', actual==predictions), axis=0)) # Ham, classified as Ham\n",
    "    FP = np.sum(np.all((actual=='ham', actual!=predictions), axis=0)) # Ham, classified as Spam\n",
    "    FN = np.sum(np.all((actual=='spam', actual!=predictions), axis=0)) # Spam, classified as Ham\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    print('Accuracy:', Accuracy)\n",
    "    print('Precision:', Precision)\n",
    "    print('Recall:', Recall)\n",
    "    return Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, omega = Pre_Process_Data('labels')\n",
    "D_train, omega_train, D_test, omega_test = Train_Test_Split(D, omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 82977\n",
      "Prior Probability Spam: 0.6581303116147309\n",
      "Prior Probability Ham: 0.3418696883852691\n"
     ]
    }
   ],
   "source": [
    "training_output = Train_Naive_Bayes(D_train, omega_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boolean list is used to speed up prediction when changing lambda smoothing\n",
    "D_test_bool = Test_Vocab(D_test, training_output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = np.zeros((5,3)) # [Accuracy, Precision, Recall]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\lambda = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9899532916189301\n",
      "Precision: 0.991207034372502\n",
      "Recall: 0.9935897435897436\n"
     ]
    }
   ],
   "source": [
    "actual_0, predictions_0 = Predict_Naive_Bayes(D_test_bool, omega_test, *training_output, smoothing=2)\n",
    "Metrics[0] = Compute_Metrics(actual_0, predictions_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9907464528069093\n",
      "Precision: 0.9938461538461538\n",
      "Recall: 0.9921207264957265\n"
     ]
    }
   ],
   "source": [
    "actual_1, predictions_1 = Predict_Naive_Bayes(D_test_bool, omega_test, *training_output, smoothing=1)\n",
    "Metrics[1] = Compute_Metrics(actual_1, predictions_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\lambda = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9893363884727241\n",
      "Precision: 0.9944959054906699\n",
      "Recall: 0.9893162393162394\n"
     ]
    }
   ],
   "source": [
    "actual_2, predictions_2 = Predict_Naive_Bayes(D_test_bool, omega_test, *training_output, smoothing=0.5)\n",
    "Metrics[2] = Compute_Metrics(actual_2, predictions_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\lambda = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9805234863840663\n",
      "Precision: 0.9896240398868077\n",
      "Recall: 0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "actual_3, predictions_3 = Predict_Naive_Bayes(D_test_bool, omega_test, *training_output, smoothing=0.1)\n",
    "Metrics[3] = Compute_Metrics(actual_3, predictions_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\lambda = 0.005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9694192297523575\n",
      "Precision: 0.97035963641154\n",
      "Recall: 0.9837072649572649\n"
     ]
    }
   ],
   "source": [
    "actual_4, predictions_4 = Predict_Naive_Bayes(D_test_bool, omega_test, *training_output, smoothing=0.005)\n",
    "Metrics[4] = Compute_Metrics(actual_4, predictions_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy, lambda=1\n",
      "Max Precision, lambda=0.5\n",
      "Max Recall, lambda=2\n"
     ]
    }
   ],
   "source": [
    "x_labels = np.array(['2', '1', '0.5', '0.1', '0.005'])\n",
    "print('Max Accuracy, lambda='+x_labels[np.argmax(Metrics[:,0])])\n",
    "print('Max Precision, lambda='+x_labels[np.argmax(Metrics[:,1])])\n",
    "print('Max Recall, lambda='+x_labels[np.argmax(Metrics[:,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mutual_Information(D, omega, num_words, reduce_words=False):\n",
    "    # Determine the top words based on Mutual Information\n",
    "    # Has the option to remove the infrequent and frequent words\n",
    "    prior_spam, prior_ham, words_spam, words_ham, n_spam, n_ham, vocab = Train_Naive_Bayes(D, omega)\n",
    "    word_occur = words_spam + words_ham\n",
    "    V = len(vocab)\n",
    "    prior_D_1 = word_occur / V # Number of times the word occur\n",
    "    prior_D_0 = 1 - prior_D_1 # Number of times the word did not occur\n",
    "    smoothing = 1\n",
    "    likelihood_spam = (words_spam + smoothing)/(n_spam + smoothing*V)\n",
    "    likelihood_ham = (words_ham + smoothing)/(n_ham + smoothing*V)\n",
    "\n",
    "    # Log Mutual Information formula\n",
    "    log_spam_1 = likelihood_spam * (np.log(likelihood_spam) - np.log(prior_D_1) - np.log(prior_spam))\n",
    "    log_spam_0 = (1 - likelihood_spam) * (np.log(1 - likelihood_spam) - np.log(prior_D_0) - np.log(prior_spam))\n",
    "    log_ham_1 = likelihood_ham * (np.log(likelihood_ham) - np.log(prior_D_1) - np.log(prior_ham))\n",
    "    log_ham_0 = (1 - likelihood_ham) * (np.log(1 - likelihood_ham) - np.log(prior_D_0) - np.log(prior_ham))\n",
    "    MI = log_spam_1 + log_spam_0 + log_ham_1 + log_ham_0\n",
    "\n",
    "    # Option to remove frequent and infrequent words\n",
    "    if reduce_words:\n",
    "        MI[np.any([word_occur < 3, word_occur > 200], axis=0)] = 0\n",
    "\n",
    "    # Get the top words\n",
    "    info_words = vocab[np.argsort(MI)[-num_words:]]\n",
    "    return info_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the top 200 words using Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 98984\n",
      "Prior Probability Spam: 0.6586642694727937\n",
      "Prior Probability Ham: 0.3413357305272064\n",
      "Top 200 informative words\n",
      " ['foxmail' 'first' 'send' 'week' 'zproxy' 'many' 'office' 'dmdx' 'contact'\n",
      " 'file' 'cs' 'into' 'them' 'information' 'oct' 'back' 'two' 'over' 'visit'\n",
      " 'dsl' 'way' 'nov' 'ra' 'its' 'fast' 'down' 'telesp' 'than' 'stock'\n",
      " 'offer' 'very' 'board' 'used' 'make' 'these' 'version' 'work' 'devmail'\n",
      " 'day' 'best' 'see' 'handyboard' 'here' 'should' 'problem' 'then' 'after'\n",
      " 'anyone' 'x' 'could' 'uid' 'need' 'want' 'invoked' 'g' 'email'\n",
      " 'university' 'jan' 'does' 'hi' 'good' 'other' 'help' 'time' 'they' 'news'\n",
      " 'z' 'br' 'know' 'mar' 'body' 'jp' 'feb' 'apr' 'effects' 'product' 'just'\n",
      " 'how' 'which' 'm' 'b' 'us' 'u' 'me' 'am' 'o' 'also' 'set' 'company' 'l'\n",
      " 'p' 'n' 'thanks' 'what' 'r' 'up' 'only' 's' 'new' 'was' 'e' 'like' 't'\n",
      " 'some' 'c' 'about' 'v' 'when' 'been' 'please' 'get' 'edua' 'campaign'\n",
      " 'sender' 'allowed' 'would' 'so' 'now' 'opt' 'more' 'there' 'apache' 'use'\n",
      " 'localhost' 'using' 'my' 'one' 'do' 'fri' 'no' 'co' 'wed' 'mon' 'psych'\n",
      " 'mail' 'thu' 'tue' 'sun' 'sat' 'arizona' 'may' 'any' 'an' 'website' 'has'\n",
      " 'public' 'out' 'as' 'html' 'but' 'normal' 'info' 'if' 'can' 'produced'\n",
      " 'we' 'list' 'express' 'mimeole' 'all' 'will' 'not' 'or' 'are' 'outlook'\n",
      " 'have' 'be' 'at' 'that' 'microsoft' 'your' 'on' 'i' 'it' 'smtp' 'our'\n",
      " 'message' 'format' 'net' 'aleve' 'mime' 'media' 'mit' 'you' 'of' 'this'\n",
      " 'and' 'the' 'is' 'in' 'com' 'to' 'a' 'esmtp' 'for' 'edu' 'id' 'with'\n",
      " 'from' 'by']\n"
     ]
    }
   ],
   "source": [
    "vocab = Mutual_Information(D, omega, 200)\n",
    "print('Top 200 informative words\\n',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 200\n",
      "Prior Probability Spam: 0.6581303116147309\n",
      "Prior Probability Ham: 0.3418696883852691\n"
     ]
    }
   ],
   "source": [
    "training_output_200 = Train_Naive_Bayes(D_train, omega_train, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boolean matrix is used to spped up prediction when changing lambda smoothing\n",
    "D_test_bool_200 = Test_Vocab(D_test, training_output_200[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.859257953644135\n",
      "Precision: 0.9128241065171688\n",
      "Recall: 0.8697916666666666\n"
     ]
    }
   ],
   "source": [
    "actual_200, predictions_200 = Predict_Naive_Bayes(D_test_bool_200, omega_test, *training_output_200, smoothing=0.5)\n",
    "Metrics_200 = Compute_Metrics(actual_200, predictions_200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing frequent and infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 98984\n",
      "Prior Probability Spam: 0.6586642694727937\n",
      "Prior Probability Ham: 0.3413357305272064\n",
      "Top 200 informative words\n",
      " ['corresponding' 'tables' 'unclaimed' 'vs' 'chris' 'bulletins' 'checking'\n",
      " 'rules' 'premium' 'evaluation' 'chuck' 'linear' 'porn' 'supported'\n",
      " 'gains' 'thread' 'accepted' 'procedure' 'ny' 'becomes' 'resolution'\n",
      " 'ericson' 'advisory' 'necessarily' 'lomas' 'corporations' 'workshop'\n",
      " 'sends' 'ject' 'ext' 'ballot' 'speedy' 'comp' 'counter' 'tatoosh'\n",
      " 'intervals' 'measured' 'frequency' 'malicious' 'chngmeds' 'baggins'\n",
      " 'winner' 'infrared' 'theoretical' 'wendy' 'encoders' 'receiver' 'convert'\n",
      " 'hu' 'wrongfully' 'caused' 'que' 'ay' 'sequence' 'maya' 'mellon' 'tom'\n",
      " 'hopefully' 'variables' 'od' 'essentially' 'vulnerabilities' 'kb'\n",
      " 'proposed' 'indigo' 'overhead' 'frontpage' 'dwarves' 'nick' 'presents'\n",
      " 'slightly' 'scale' 'electrical' 'banking' 'jcf' 'dhcp' 'occurs' 'weeek'\n",
      " 'finphame' 'univers' 'instance' 'fails' 'reasonable' 'nut' 'pres'\n",
      " 'brasiltelecom' 'laboratory' 'pio' 'sieve' 'anotherparty' 'loading' 'psy'\n",
      " 'background' 'suggestion' 'remitted' 'gilfoyle' 'coordination' 'jim'\n",
      " 'audio' 'montana' 'java' 'ticks' 'erection' 'cluster' 'laptop' 'teaching'\n",
      " 'seeing' 'deimos' 'station' 'pictures' 'finished' 'tabs' 'boot' 'faculty'\n",
      " 'attacker' 'wen' 'latency' 'rees' 'crown' 'turns' 'ttnet' 'nowrap'\n",
      " 'modify' 'query' 'alt' 'ities' 'pwm' 'experimental' 'packet' 'packets'\n",
      " 'enhancement' 'equipment' 'bytes' 'mechanism' 'mba' 'livedoor' 'images'\n",
      " 'interrupt' 'tigious' 'vladik' 'mouse' 'buffer' 'pair' 'drugs' 'compiler'\n",
      " 'experiments' 'revision' 'pills' 'rob' 'timeout' 'revenues' 'icb'\n",
      " 'vulnerability' 'explode' 'todays' 'lucrative' 'fbx' 'depends'\n",
      " 'disqualify' 'displays' 'holdingsadvanced' 'redcasino' 'variable'\n",
      " 'character' 'bachelors' 'logic' 'loop' 'sunos' 'connections' 'solved'\n",
      " 'connecting' 'tc' 'utep' 'hosts' 'matt' 'groupms' 'expressed' 'implement'\n",
      " 'plug' 'keyboard' 'objects' 'illustrator' 'lenders' 'robots' 'relatively'\n",
      " 'specified' 'iceman' 'queries' 'neoplus' 'pppoe' 'servos' 'arbitrary'\n",
      " 'sendmail' 'sparc' 'ixed' 'lego' 'umt' 'batteries' 'namidaame'\n",
      " 'majanmagazine']\n"
     ]
    }
   ],
   "source": [
    "vocab = Mutual_Information(D, omega, 200, reduce_words=True)\n",
    "print('Top 200 informative words\\n',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 200\n",
      "Prior Probability Spam: 0.6581303116147309\n",
      "Prior Probability Ham: 0.3418696883852691\n"
     ]
    }
   ],
   "source": [
    "training_output_200 = Train_Naive_Bayes(D_train, omega_train, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boolean matrix is used to spped up prediction when changing lambda smoothing\n",
    "D_test_bool_200 = Test_Vocab(D_test, training_output_200[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8817308539702123\n",
      "Precision: 0.8560834298957126\n",
      "Recall: 0.9866452991452992\n"
     ]
    }
   ],
   "source": [
    "actual_200, predictions_200 = Predict_Naive_Bayes(D_test_bool_200, omega_test, *training_output_200, smoothing=0.5)\n",
    "Metrics_200 = Compute_Metrics(actual_200, predictions_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_File(file_name, variable):\n",
    "    with open(file_name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(variable)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_File('D_train.csv', D_train)\n",
    "Save_File('omega_train.csv', omega_train)\n",
    "Save_File('D_test.csv', D_test)\n",
    "Save_File('omega_test.csv', omega_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
